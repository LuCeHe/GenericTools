import os
import matplotlib.pyplot as plt

def plot_history(history, plot_filename, epochs):
    if epochs > 0:
        # plot training and validation losses
        fig = plt.figure(figsize=(8, 8))
        ax = fig.add_subplot(111)
        ax.plot(history.history['loss'], label='train')
        ax.plot(history.history['val_loss'], label='val')
        ax.set_title('model loss')
        ax.set_ylabel('loss')
        ax.set_xlabel('epoch')
        ax.legend()
        fig.savefig(plot_filename, bbox_inches='tight')




if __name__ == '__main__':

    loss = [1.0477011, 1.0083296, 0.9801555, 0.9529059, 0.9089482, 0.8679432, 0.83369905, 0.79718214, 0.77358377, 0.75764084, 0.74460655, 0.7405122, 0.7377305, 0.7280034, 0.7187063, 0.7085081, 0.6895534, 0.6709108, 0.6567369, 0.6393268, 0.62461525, 0.61569536, 0.60410005, 0.5979619, 0.5976902, 0.59165704, 0.5863923, 0.5847984, 0.5763996, 0.56934536, 0.5673612, 0.5605501, 0.554474, 0.5526762, 0.54574144, 0.53903496, 0.5369735, 0.5313107, 0.52552927, 0.52318645, 0.51686794, 0.51032627, 0.507888, 0.5027636, 0.49751243, 0.4957912, 0.49191898, 0.48774382, 0.48561278, 0.48105824, 0.4762799, 0.47391498, 0.47008497, 0.46579987, 0.46339643, 0.45934284, 0.45413724, 0.4508839, 0.4473047, 0.4429398, 0.4400119, 0.43687373, 0.43219557, 0.42897925, 0.42616728, 0.4217105, 0.41840863, 0.41547903, 0.4105617, 0.40677616, 0.40413773, 0.39958447, 0.39574027, 0.39329964, 0.3886608, 0.38496408, 0.3831625, 0.37855682, 0.3746426, 0.37280416, 0.36840737, 0.36488223, 0.3635468, 0.35953492, 0.35628915, 0.35559198, 0.35178763, 0.34866276, 0.3480156, 0.34447354, 0.3413755, 0.34087935, 0.33741125, 0.33430198, 0.33415458, 0.33061984, 0.32757688, 0.32724842, 0.32442656, 0.3211521, 0.3210336, 0.3179615, 0.31507048, 0.31496692, 0.31213954, 0.30919716, 0.30912718, 0.30654594, 0.3036544, 0.3037514, 0.30115342, 0.29839742, 0.2985545, 0.2962353, 0.29342, 0.29363653, 0.29128227, 0.28873205, 0.2888608, 0.2864981, 0.2840762, 0.28419438, 0.28187752, 0.27930167, 0.28003457, 0.27740785, 0.2752403, 0.2755938, 0.2734213, 0.27117565, 0.27202573, 0.26964375, 0.2675609, 0.2682707, 0.26642036, 0.26426706, 0.26499358, 0.2632061, 0.26109242, 0.2619424, 0.2601346, 0.25802025, 0.25927195, 0.25709137, 0.25513464, 0.25637648, 0.25462195, 0.2525487, 0.25322437, 0.2524095, 0.250073, 0.25195813, 0.24979614, 0.24745995, 0.24849962, 0.24811858, 0.24529572, 0.24724966, 0.24555753, 0.24318746, 0.24441552, 0.24457712, 0.24102472, 0.24240044, 0.2426937, 0.23942655, 0.24032904, 0.2406324, 0.23799244, 0.23816267, 0.23945113, 0.23656672, 0.2360651, 0.2365678, 0.23589654, 0.23421265, 0.23547228, 0.23482658, 0.2327955, 0.23203434, 0.23385343, 0.23237221, 0.22980487, 0.23161003, 0.23234518, 0.22789526, 0.22868192, 0.23166168, 0.22725616, 0.22591609, 0.2298264, 0.22756752, 0.22343992, 0.22711691, 0.22715947, 0.22283223, 0.22391213, 0.22583574, 0.22371745, 0.22140259, 0.22323057, 0.22314267, 0.22100003, 0.21986642, 0.22190008, 0.22148694, 0.2178325, 0.21894556, 0.22179054, 0.21757354, 0.21548875, 0.21966751, 0.21886852, 0.214393, 0.21580541, 0.21831204, 0.21558541, 0.21375793, 0.21512872, 0.21663828, 0.21337382, 0.21215421, 0.21434885, 0.21503153, 0.21075056, 0.21094747, 0.21483786, 0.21155207, 0.20882472, 0.21161015, 0.21247432, 0.2086336, 0.20909294, 0.21098799, 0.20964998, 0.20804952, 0.2082639, 0.20916876, 0.20865503, 0.20609736, 0.2073552, 0.2086086, 0.20671707, 0.20496175, 0.20752124, 0.20674776, 0.20407543, 0.20575511, 0.206161, 0.20463909, 0.20377345, 0.20448937, 0.20459569, 0.20390321, 0.20252348, 0.20349681, 0.20447321, 0.20185415, 0.20146072, 0.20401624, 0.20218179, 0.20022355, 0.20237198, 0.20243503, 0.20032689, 0.20052822, 0.2016613, 0.20046872, 0.19982007, 0.19963157, 0.19998458, 0.19993469, 0.19847561, 0.198948, 0.19981879, 0.19826049, 0.19739084, 0.19935074, 0.19830887, 0.19662091, 0.1980215, 0.19803508, 0.19670229, 0.19693169, 0.19717465, 0.19682829, 0.19630852, 0.19595519, 0.19637358, 0.19656345, 0.19515692, 0.1949727, 0.19715561, 0.19492552, 0.19436789, 0.19572946, 0.19536944, 0.1934611, 0.19515386, 0.194899, 0.19357857, 0.19389634, 0.1944578, 0.19364484, 0.19339465, 0.19338933, 0.19336803, 0.19330497, 0.19226554, 0.19278787, 0.19343138, 0.19187734, 0.19150898, 0.19335993, 0.19179201, 0.19071335, 0.19243896, 0.1920884, 0.19033979, 0.19130644, 0.1920593, 0.19015218, 0.19061731, 0.19117725, 0.1907278, 0.19021036, 0.18996753, 0.19047923, 0.19025436, 0.18927309, 0.18951984, 0.19057794, 0.18891816, 0.18861431, 0.19014648, 0.18903702, 0.18786216, 0.1893865, 0.18903168, 0.18757322, 0.18867005, 0.18858552, 0.18782534, 0.18787009, 0.18823616, 0.18752186, 0.18751389, 0.18757749, 0.18717161, 0.18742004, 0.18660226, 0.1868663, 0.18733338, 0.18618573, 0.18602797, 0.18735905, 0.18599224, 0.18531643, 0.18683822, 0.18606779, 0.18484849, 0.18610193, 0.18614978, 0.1846689, 0.18544345, 0.18569115, 0.1847342, 0.18486717, 0.18516485, 0.18452333, 0.18466046, 0.18452339, 0.18433233, 0.18458492, 0.18377714, 0.18405548, 0.1845067, 0.18342964, 0.18332751, 0.18444827, 0.18322025, 0.18259588, 0.18413232, 0.18320878, 0.18221731, 0.18359086, 0.18305057, 0.18200067, 0.1828647, 0.18297301, 0.18182887, 0.18229501, 0.18258992, 0.18181884, 0.1819389, 0.18198448, 0.1817703, 0.18172655, 0.18139015, 0.18158041, 0.18161833, 0.18092008, 0.1810206, 0.18162951, 0.18060623, 0.18030952, 0.1817377, 0.18041217, 0.17987029, 0.18125336, 0.18034777, 0.17949547, 0.1807247, 0.18036711, 0.1792118, 0.18006061, 0.1806005, 0.17896414, 0.17961198, 0.18006323, 0.17924649, 0.17934372, 0.17921942, 0.17961358, 0.17897914, 0.17916736, 0.17863047, 0.1789558, 0.17863005, 0.17858228, 0.17891023, 0.17811406, 0.17825125, 0.17879634, 0.17776018, 0.17775051, 0.17871176, 0.17766932, 0.17725264, 0.17843126, 0.1776594, 0.17674345, 0.1781604, 0.1775762, 0.1764764, 0.17775513, 0.17729157, 0.17644486, 0.17714171, 0.17723586, 0.17636469, 0.17663017, 0.17697084, 0.17635423, 0.17629054, 0.17667717, 0.17608629, 0.17623599, 0.17607875, 0.17628388, 0.17602172, 0.1754947, 0.17615072, 0.17597702, 0.17520443, 0.17547889, 0.17634176, 0.17494228, 0.17484601, 0.1763359, 0.17476171, 0.17431605, 0.17596132, 0.17528082, 0.17409457, 0.17519887, 0.17547326, 0.17376786, 0.1749786, 0.1748233, 0.1738772, 0.1744377, 0.1748669, 0.17379147, 0.17394659, 0.1745467, 0.17390442, 0.17365488, 0.17386734, 0.1742216, 0.17349884, 0.17334713, 0.17381115, 0.17382151, 0.17305896, 0.17311792, 0.18378156, 0.24704865, 0.24092928, 0.22943874, 0.23001447, 0.23537014, 0.23645155, 0.23103064, 0.22503857, 0.22048931, 0.2225345, 0.2237624, 0.22099304, 0.21725331, 0.21465656, 0.21072683, 0.21247886, 0.21265866, 0.20834474, 0.20641586, 0.20572376, 0.20432357, 0.20427233, 0.20502353, 0.20055176, 0.19954287, 0.20075607, 0.19963747, 0.19773217, 0.19814609, 0.19606534, 0.19428854, 0.19533762, 0.1947566, 0.19289535, 0.19234073, 0.19238818, 0.19124344, 0.19046447, 0.19099723, 0.19051448, 0.1886579, 0.18818934, 0.18944605, 0.18749863, 0.18676117, 0.18825099, 0.18701935, 0.18446887, 0.18681306, 0.18651903, 0.18324925, 0.18574335, 0.18643245, 0.18255742, 0.18304025, 0.18664482, 0.18238473, 0.18207717, 0.18571985, 0.18369135, 0.17977215, 0.18392742, 0.18443868, 0.17927292, 0.18354896, 0.18424985, 0.18126558, 0.17877711, 0.18493709, 0.18172106, 0.1774632, 0.18304543, 0.18352285, 0.17941763, 0.17702012, 0.1845072, 0.18145144, 0.17541109, 0.18180797, 0.184139, 0.17845482, 0.17497477, 0.18284275, 0.18249519, 0.17440069, 0.17944966, 0.18436883, 0.18008588, 0.17397143, 0.17820019, 0.18515909, 0.17698157, 0.17374183, 0.18234499, 0.1835098, 0.17627172, 0.17238358, 0.18157731, 0.18219134, 0.17289713, 0.17510475, 0.18207285, 0.18151088, 0.17366312, 0.17184892, 0.18015377, 0.18052706, 0.1712832, 0.17371064, 0.18062761, 0.1793277, 0.17267814, 0.17106555, 0.17854232]

    plot_filename = 'train_losses.pdf'
    fig = plt.figure(figsize=(8, 8))
    ax = fig.add_subplot(111)
    ax.plot(loss, label='snd2snd')
    ax.set_title('model loss')
    ax.set_ylabel('loss')
    ax.set_xlabel('epoch')
    ax.legend()
    fig.savefig(plot_filename, bbox_inches='tight')
